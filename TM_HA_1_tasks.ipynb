{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining - Assignment 1 (30 points total)\n",
    "This **Home Assignment** is to be submitted and you will be given points for each of the tasks. \n",
    "You can use the nltkand all of the python standard library. Assume the code is run under python 3.7.\n",
    "\n",
    "## Formalities\n",
    "**Submit in a group of 3-4 people until 24.11.2020 23:59CET. The deadline is strict!**\n",
    "\n",
    "## Evaluation and Grading\n",
    "General advice for programming excercises at *CSSH*:\n",
    "Evaluation of your submission is done semi automatically. Think of it as this notebook being \n",
    "executed once. Afterwards, some test functions are appended to this file and executed respectively.\n",
    "\n",
    "Therefore:\n",
    "* Submit valid _Python3_ code only!\n",
    "* Use external libraries only when specified by task.\n",
    "* Ensure your definitions (functions, classes, methods, variables) follow the specification if\n",
    "  given. The concrete signature of e.g. a function usually can be inferred from task description, \n",
    "  code skeletons and test cases.\n",
    "* Ensure the notebook does not rely on current notebook or system state!\n",
    "  * Use `Kernel --> Restart & Run All` to see if you are using any definitions, variables etc. that \n",
    "    are not in scope anymore.\n",
    "* Keep your code idempotent! Running it or parts of it multiple times must not yield different\n",
    "  results. Minimize usage of global variables.\n",
    "* Ensure your code / notebook terminates in reasonable time.\n",
    "\n",
    "**There's a story behind each of these points! Don't expect us to fix your stuff!**\n",
    "\n",
    "Regarding the scores, you will get no points for a task if:\n",
    "- your function throws an unexpected error (e.g. takes the wrong number of arguments)\n",
    "- gets stuck in an infinite loop\n",
    "- takes much much longer than expected (e.g. >1s to compute the mean of two numbers)\n",
    "- does not produce the desired output (e.g. returns an descendingly sorted list even though we asked for ascending, returns the mean and the std even though we asked for only the mean, prints an output instead of returning it, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credentials of all team members\n",
    "team_members = [\n",
    "    {\n",
    "        'first_name': 'Alice',\n",
    "        'last_name': 'Foo',\n",
    "        'student_id': 12345\n",
    "    },\n",
    "    {\n",
    "        'first_name': 'Bob',\n",
    "        'last_name': 'Bar',\n",
    "        'student_id': 54321\n",
    "    },\n",
    "    {\n",
    "        'first_name': 'Some',\n",
    "        'last_name': 'One',\n",
    "        'student_id': 11122\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Finding the most similar word (12 points)\n",
    "\n",
    "The goal of this task is given a corpus find the most similar word for a provided word. As an example we will consider the `'bible-kjv.txt'`corpus where we are looking to find the word that is most similar to `god`. We consider two words similar if they appear in the same context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Cleaning the input (3)\n",
    "\n",
    "Write a function `get_valid_tokens(corpus, remove_stopwords=False)` that given a list of tokens returns a list of tokens that we consider valid for out task.\n",
    "\n",
    "An *invalid* token is a token that \n",
    "- is a punctuation mark (consider `string.puctuation`).\n",
    "- is entirely numeric digits (e.g. `\"123\"`)\n",
    "- optionally if `remove_stopwords=True` stopwords in the englisch language are also not considered valid tokens (use nltk's stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "\n",
    "def get_valid_tokens(corpus, remove_stopwords=False):\n",
    "    return []\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "l=[]\n",
    "for i, word in enumerate(get_valid_tokens(nltk.corpus.gutenberg.words('bible-kjv.txt'))):\n",
    "    l.append(word)\n",
    "    if i >50:\n",
    "        break\n",
    "print(l)\n",
    "#['the', 'king', 'james', 'bible', 'the', 'old', 'testament', 'of', 'the', 'king', 'james', 'bible', 'the', 'first', 'book', 'of', 'moses', 'called', 'genesis', 'in', 'the', 'beginning', 'god', 'created', 'the', 'heaven', 'and', 'the', 'earth', 'and', 'the', 'earth', 'was', 'without', 'form', 'and', 'void', 'and', 'darkness', 'was', 'upon', 'the', 'face', 'of', 'the', 'deep', 'and', 'the', 'spirit', 'of', 'god', 'moved']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Counting the sorroundings (4)\n",
    "\n",
    "In our simple model of word similarity we consider words as similar if they are being used in the same context (i.e. they have similar words sorrounding them).\n",
    "Therefor firstly write a function `get_sorrounding_counts(corpus, context_size)` that given a corpus computes the count of all words in a designated word's sorrounding. The context size indicates how far to the left and right we go. I.e. context size of 1 indicates we only consider the words immediately before and after to be in the context of the central word.\n",
    "\n",
    "It is a dictionary which maps a word onto a dictionary. This inner dictionary maps words onto counts.\n",
    "\n",
    "Maybe an example is most easy to understand what we want to compute. For the input\n",
    "`['hi', 'james', 'how', 'are', 'you', 'hello', 'catherine', 'how', 'are', 'you']` and `context_size=1`\n",
    "we whish to obtain:\n",
    "```\n",
    "{'hi': {'james': 1},\n",
    " 'james': {'hi': 1, 'how': 1},\n",
    " 'how': {'james': 1, 'are': 2, 'catherine': 1},\n",
    " 'are': {'how': 2, 'you': 2},\n",
    " 'you': {'are': 2, 'hello': 1},\n",
    " 'hello': {'you': 1, 'catherine': 1},\n",
    " 'catherine': {'hello': 1, 'how': 1}}\n",
    "```\n",
    "Explanation of the output: The word hi is only surrounded by james. james is sorrounded by hi and how. how is surrounded by james, by are twice and by catherine, ...\n",
    "\n",
    "For `contextsize=2` we obtain:\n",
    "```\n",
    "{'hi': Counter({'james': 1, 'how': 1}),\n",
    "'james': Counter({'hi': 1, 'how': 1, 'are': 1}),\n",
    "'how': Counter({'are': 2, 'you': 2, 'hi': 1, 'james': 1, 'hello': 1, 'catherine': 1}),\n",
    "'are': Counter({'how': 2, 'you': 2, 'james': 1, 'hello': 1, 'catherine': 1}),\n",
    "'you': Counter({'how': 2, 'are': 2, 'hello': 1, 'catherine': 1}),\n",
    "'hello': Counter({'are': 1, 'you': 1, 'catherine': 1, 'how': 1}),\n",
    "'catherine': Counter({'you': 1, 'hello': 1, 'how': 1, 'are': 1})}\n",
    "```\n",
    "\n",
    "Hint: Instead of the inner dictionary you can alternatively use `Counter` or `defaultdict` which are in `collections`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sorrounding_counts(corpus, context_size):\n",
    "    return {'solomon':{}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'solomon': {}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_text = ['hi', 'james', 'how', 'are', 'you', 'hello', 'catherine', 'how', 'are', 'you']\n",
    "get_sorrounding_counts(simple_text, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'solomon': {}}\n"
     ]
    }
   ],
   "source": [
    "print(get_sorrounding_counts(simple_text, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# now for the bible corpus\n",
    "tokens = get_valid_tokens(nltk.corpus.gutenberg.words('bible-kjv.txt'), remove_stopwords=False)\n",
    "d=get_sorrounding_counts(tokens, 2)\n",
    "print(len(d)) #12579"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "print(d['solomon'])\n",
    "#{'and': 123, 'king': 63, 'of': 62, 'the': 49, 'to': 49, 'son': 37, 'had': 25, 's': 25, 'made': 24, 'all': 20, 'for': 19, 'unto': 19, 'which': 19, 'his': 17, 'that': 17, 'then': 16, 'in': 15, 'built': 12, 'said': 11, 'gave': 11, 'did': 9, 'a': 9, 'lord': 9, 'servants': 9, 'my': 8, 'sent': 8, 'solomon': 8, 'wisdom': 8, 'when': 8, 'saying': 7, 'thy': 7, 'he': 7, 'with': 7, 'was': 7, 'so': 7, 'told': 6, 'days': 6, 'came': 6, 'but': 5, 'out': 5, 'hiram': 5, 'work': 5, 'brought': 5, 'children': 5, 'is': 5, 'also': 4, 'servant': 4, 'make': 4, 'they': 4, 'because': 4, 'behold': 4, 'she': 4, 'wife': 4, 'reigned': 4, 'year': 4, 'israel': 4, 'stood': 4, 'time': 4, 'years': 4, 'her': 4, 'went': 4, 'nathan': 3, 'assuredly': 3, 'shall': 3, 'be': 3, 'hath': 3, 'than': 3, 'thee': 3, 'by': 3, 'appeared': 3, 'thou': 3, 'daughter': 3, 'from': 3, 'david': 3, 'cedar': 3, 'thus': 3, 'house': 3, 'before': 3, 'offered': 3, 'finished': 3, 'come': 3, 'heart': 3, 'horses': 3, 'father': 3, 'acts': 3, 'gold': 3, 'them': 3, 'proverbs': 3, 'ibhar': 2, 'name': 2, 'mother': 2, 'save': 2, 'on': 2, 'let': 2, 'sat': 2, 'upon': 2, 'altar': 2, 'hand': 2, 'loved': 2, 'offer': 2, 'over': 2, 'god': 2, 'i': 2, 'desire': 2, 'as': 2, 'raised': 2, 'levy': 2, 'threescore': 2, 'officers': 2, 'concerning': 2, 'it': 2, 'like': 2, 'assembled': 2, 'jerusalem': 2, 'spake': 2, 'pass': 2, 'cities': 2, 'desired': 2, 'no': 2, 'were': 2, 'burnt': 2, 'fame': 2, 'communed': 2, 'one': 2, 'two': 2, 'sought': 2, 'hear': 2, 'gathered': 2, 'abundance': 2, 'slept': 2, 'four': 2, 'temple': 2, 'called': 2, 'exceedingly': 2, 'up': 2, 'wherein': 2, 'people': 2, 'night': 2, 'presence': 2, 'rehoboam': 2, 'not': 2, 'song': 2, 'begat': 2, 'greater': 2, 'here': 2, 'porch': 2, 'men': 1, 'brother': 1, 'hast': 1, 'go': 1, 'host': 1, 'cause': 1, 'ye': 1, 'caused': 1, 'ride': 1, 'anointed': 1, 'sitteth': 1, 'better': 1, 'arose': 1, 'feareth': 1, 'lo': 1, 'swear': 1, 'sword': 1, 'if': 1, 'charged': 1, 'speak': 1, 'answered': 1, 'sware': 1, 'afflicted': 1, 'thrust': 1, 'joab': 1, 'benaiah': 1, 'shimei': 1, 'affinity': 1, 'offerings': 1, 'asked': 1, 'awoke': 1, 'tribute': 1, 'twelve': 1, 'merry': 1, 'served': 1, 'life': 1, 'provision': 1, 'forty': 1, 'table': 1, 'shore': 1, 'words': 1, 'trees': 1, 'chief': 1, 'builders': 1, 'reign': 1, 'overlaid': 1, 'building': 1, 'wrought': 1, 'zarthan': 1, 'left': 1, 'at': 1, 'egypt': 1, 'held': 1, 'second': 1, 'furnished': 1, 'given': 1, 'gezer': 1, 'store': 1, 'those': 1, 'seen': 1, 'drinking': 1, 'exceeded': 1, 'together': 1, 'many': 1, 'their': 1, 'gods': 1, 'clave': 1, 'old': 1, 'after': 1, 'ammonites': 1, 'evil': 1, 'build': 1, 'an': 1, 'angry': 1, 'forasmuch': 1, 'adversary': 1, 'hadad': 1, 'beside': 1, 'zereda': 1, 'millo': 1, 'valour': 1, 'seeing': 1, 'will': 1, 'ever': 1, 'therefore': 1, 'death': 1, 'jeroboam': 1, 'corruption': 1, 'bases': 1, 'sister': 1, 'singing': 1, 'until': 1, 'brass': 1, 'wherewith': 1, 'help': 1, 'chosen': 1, 'me': 1, 'congregation': 1, 'give': 1, 'priest': 1, 'themselves': 1, 'magnified': 1, 'honour': 1, 'chronicles': 1, 'fathers': 1, 'appear': 1, 'this': 1, 'chariots': 1, 'means': 1, 'determined': 1, 'kingdom': 1, 'numbered': 1, 'began': 1, 'things': 1, 'instructed': 1, 'zeredathah': 1, 'hands': 1, 'now': 1, 'moreover': 1, 'hallowed': 1, 'same': 1, 'kept': 1, 'into': 1, 'restored': 1, 'there': 1, 'prepared': 1, 'eziongeber': 1, 'ophir': 1, 'prove': 1, 'hard': 1, 'hid': 1, 'silver': 1, 'passed': 1, 'first': 1, 'nebat': 1, 'strong': 1, 'three': 1, 'writing': 1, 'wise': 1, 'curtains': 1, 'look': 1, 'himself': 1, 'found': 1, 'favour': 1, 'o': 1, 'must': 1, 'have': 1, 'urias': 1, 'roboam': 1, 'even': 1, 'you': 1, 'greatly': 1, 'accord': 1, 'jacob': 1, 'him': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Keep top k words in context (2)\n",
    "\n",
    "To reduce the size of the context we will only consider the most frequent words for each context. Therefor write a function `to_sets(count_dicts, k)` that for each word only keeps the `k` most frequent words of the context. Ties are resolved in favor of the lexicographically lower word (comes earlier when sorting the words). It returns a dict that maps words onto their context words (which are now stored in a set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sets(count_dicts, k):\n",
    "    return {'lord' : {'hi'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "{'hi'}\n"
     ]
    }
   ],
   "source": [
    "d2 = to_sets(d,20)\n",
    "print(len(d2)) #12579\n",
    "print(d2['lord'])\n",
    "# {'said', 'is', 'my', 'unto', 'o', 'god', 'before', 'and', 'that', 'to', 'in', 'saith', 'i', 'hath', 'thy', 'shall', 'of', 'for', 'the', 'which'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lord': {'hi'}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_sets(get_sorrounding_counts(simple_text, 2),2)\n",
    "\n",
    "#{'hi': {'how', 'james'},\n",
    "# 'james': {'are', 'hi'},\n",
    "# 'how': {'are', 'you'},\n",
    "# 'are': {'how', 'you'},\n",
    "# 'you': {'are', 'how'},\n",
    "# 'hello': {'are', 'catherine'},\n",
    "# 'catherine': {'are', 'hello'}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d)  Find the most similar word by context overlap (3)\n",
    "\n",
    "Given a dictionary as obtained in the previous task, and a word that you are interested in, we will now find the most similar word by Jaccard index. The definition of the Jaccard similarity for two sets can be found on wikipedia.\n",
    "Write a function `find_most_similar_word(input_word, contexts)` that returns the most similar word to the input word (which is not the input word). It returns this word and the Jaccard similarity in this order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_similar_word(input_word, contexts):\n",
    "    return 'hello', 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('hello', 0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_most_similar_word('god', d2)\n",
    "# ('lord', 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('hello', 0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_most_similar_word('land', d2)\n",
    "# ('place', 0.5384615384615384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('hello', 0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_most_similar_word('angel', d2)\n",
    "# ('ascending', 0.34615384615384615)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: WordNet path similarity (6 points)\n",
    "\n",
    "Reimplement the path similarity between two words in WordNet using the NLTK package. The path similarity between two words is given by\n",
    "$$\n",
    "\\frac{1}{1+d}\n",
    "$$\n",
    "where $d$ is the shortest path of any pair of their synsets.\n",
    "Obviously, do not use the pathsim function already implemented. From NLTK you should only use the `synsets`, `hypernyms` and `instance_hpyernyms` functions.\n",
    "\n",
    "Write a function `get_dist(w1, w2, wn_instance)` where w1, w2 are words as strings and wn is a wordnet instance that should be used. I.e. call `wn_instance.synsets()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dist(w1, w2, wn_instance):\n",
    "    path_distance = 0\n",
    "\n",
    "        \n",
    "    return 1/(1+path_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Stamm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download(\"wordnet\")\n",
    "print(get_dist(\"bank\", \"coin\", wn)) # 0.08333333333333333\n",
    "print(get_dist(\"bank\", \"money\",wn)) # 0.14285714285714285\n",
    "print(get_dist(\"bank\", \"flood\",wn)) # 0.25\n",
    "\n",
    "# Think about why bank might be most similar to flood according to this metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Markov chains (12 points)\n",
    "\n",
    "In this task we are looking to utilize the idea of Markov models and ngrams to solve different tasks. Throughout this task we will define a single class with different functions. You can certainly reuse the functions from earlier tasks. Whenever I use the term past, I mean an ngram represented as a tuple of strings.\n",
    "\n",
    "## a) Collecting the counts (3)\n",
    "Write a function `process_corpus` that takes a corpus (as an iterator of strings) and collects for each the \"past\" of length `order` the count that a word follows this past.\n",
    "Remember that we will need these counts for the conditional probabilities: `P(word | past)`.\n",
    "Additionally also return the entire vocabulary (i.e. all tokens that are in the corpus).\n",
    "\n",
    "## b) Conditional probabilities (4)\n",
    "Write a function `transition_prob(self, past, word)` that takes the past (a tuple of strings) and a word as input and returns the conditional probability that the word follows that past. Thereby use the laplace correction.\n",
    "If the past has never been observed return 1/size_of_vocabulary.\n",
    "\n",
    "## c) Most likely word (3)\n",
    "Write a function that given a past returns the most likely word that comes next as a list of strings. In case there are multiple equally likely words return all of them. \n",
    "Note that you do not have to loop over the entire vocabulary to obtain the most likely word.\n",
    "\n",
    "## d) Generating text using markov model (2)\n",
    "\n",
    "Write a funciton `generate_text(self, past, n)` that generates text given a starting sequence of words (`past`). It generates text by always choosing the most likely next word. It returns a list of strings. The list is of length `n`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "class MarkovModel():\n",
    "    def __init__(self, corpus, order, laplace=0):\n",
    "        self.laplace = laplace\n",
    "        self.order = order\n",
    "        self.counts, self.v = self.process_corpus(corpus)\n",
    "        \n",
    "    def process_corpus(self, corpus):\n",
    "        counts = defaultdict(Counter)\n",
    "        v=set()\n",
    "        return counts, v\n",
    "        \n",
    "    def transition_prob(self, past, word):\n",
    "        return 0.0\n",
    "    \n",
    "    def most_likely_word(self, past):\n",
    "        return ['hello', 'world']\n",
    "    \n",
    "    def generate_text(self, past, n):\n",
    "        return ['hello'] * n\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n",
      "defaultdict(<class 'collections.Counter'>, {})\n"
     ]
    }
   ],
   "source": [
    "mm = MarkovModel(get_valid_tokens(simple_text),1)\n",
    "print(mm.v) # {'how', 'hi', 'you', 'hello', 'james', 'are', 'catherine'}\n",
    "print(mm.counts)\n",
    "#{('hi',): Counter({'james': 1}), ('james',): Counter({'how': 1}), ('how',): Counter({'are': 2}), ('are',): Counter({'you': 2}), ('you',): Counter({'hello': 1}), ('hello',): Counter({'catherine': 1}), ('catherine',): Counter({'how': 1})}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n",
      "defaultdict(<class 'collections.Counter'>, {})\n"
     ]
    }
   ],
   "source": [
    "mm = MarkovModel(get_valid_tokens(simple_text),2)\n",
    "print(mm.v) # {'how', 'hi', 'you', 'hello', 'james', 'are', 'catherine'}\n",
    "print(mm.counts)\n",
    "# {('hi', 'james'): Counter({'how': 1}), ('james', 'how'): Counter({'are': 1}), ('how', 'are'): Counter({'you': 2}), ('are', 'you'): Counter({'hello': 1}), ('you', 'hello'): Counter({'catherine': 1}), ('hello', 'catherine'): Counter({'how': 1}), ('catherine', 'how'): Counter({'are': 1})}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now applied to the bible dataset\n",
    "tokens = get_valid_tokens(nltk.corpus.gutenberg.words('bible-kjv.txt'), remove_stopwords=False)\n",
    "m=MarkovModel(tokens, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.transition_prob(('in', 'the'), 'beginning')\n",
    "# 0.003379049890677798"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.transition_prob(('in', 'the'), 'land')\n",
    "# 0.06440071556350627"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'world']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.most_likely_word(('there', 'is'))\n",
    "# ['no']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = get_valid_tokens(nltk.corpus.gutenberg.words('bible-kjv.txt'), remove_stopwords=False)\n",
    "m=MarkovModel(tokens, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'world']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.most_likely_word(('the', 'lord', 'is')) # ['with']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'hello', 'hello', 'hello', 'hello', 'hello', 'hello', 'hello', 'hello', 'hello', 'hello', 'hello', 'hello', 'hello', 'hello', 'hello', 'hello', 'hello', 'hello', 'hello', 'hello', 'hello', 'hello', 'hello', 'hello', 'hello', 'hello', 'hello', 'hello', 'hello']\n"
     ]
    }
   ],
   "source": [
    "print(m.generate_text(('we', 'are', 'here'), 30))\n",
    "# ['we', 'are', 'here', 'in', 'a', 'desert', 'land', 'and', 'in', 'the', 'days', 'of', 'his', 'life', 'and', 'the', 'life', 'of', 'the', 'flesh', 'of', 'the', 'sacrifice', 'of', 'the', 'peace', 'offerings', 'and', 'the', 'priest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'hello', 'hello', 'hello', 'hello', 'hello', 'hello', 'hello', 'hello', 'hello', 'hello', 'hello', 'hello', 'hello', 'hello', 'hello', 'hello', 'hello', 'hello', 'hello']\n"
     ]
    }
   ],
   "source": [
    "print(m.generate_text(('the', 'lord', 'is'),20))\n",
    "# ['the', 'lord', 'is', 'with', 'thee', 'and', 'thou', 'shalt', 'make', 'the', 'staves', 'of', 'shittim', 'wood', 'and', 'overlay', 'them', 'with', 'gold', 'and']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
